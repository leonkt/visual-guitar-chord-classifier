{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "DISABLE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 32\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Declare important file paths\n",
    "notebook_path = os.path.abspath(\"Classifier_TL.ipynb\")\n",
    "data_path = os.path.dirname(notebook_path) + '/Workflow/Official Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Running on CPU!\n"
    }
   ],
   "source": [
    "# Select accelerator device\n",
    "def get_default_device():\n",
    "    if not DISABLE_CUDA and torch.cuda.is_available():\n",
    "        print(\"Running on CUDA!\")\n",
    "        return torch.device('cuda'), True\n",
    "    else:\n",
    "        print(\"Running on CPU!\")\n",
    "        return torch.device('cpu'), False\n",
    "device, using_cuda = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['C', 'D', 'Em', 'F', 'G']\n"
    }
   ],
   "source": [
    "# Transform the data\n",
    "transform = transforms.Compose([\n",
    "                    transforms.Resize((input_dim, input_dim)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Create training/testing dataloaders\n",
    "full_set = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "train_size = int(train_test_ratio * len(full_set))\n",
    "val_size = int((len(full_set) - train_size) / 2)\n",
    "test_size = len(full_set) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(full_set, [train_size, val_size, test_size])\n",
    "\n",
    "dataset_sizes = {'train': train_size,\n",
    "                 'val': val_size,\n",
    "                 'test': test_size}\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(train_set, shuffle=True),\n",
    "               'val': torch.utils.data.DataLoader(val_set, shuffle=False),\n",
    "               'test': torch.utils.data.DataLoader(test_set, shuffle=False)}\n",
    "\n",
    "class_names = full_set.classes\n",
    "print (class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    train_accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'): \n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':  # backward + optimize only if in training phase\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_accuracy_list.append(epoch_acc)\n",
    "                train_loss_list.append(epoch_loss)\n",
    "            else:\n",
    "                val_accuracy_list.append(epoch_acc)\n",
    "                val_loss_list.append(epoch_loss)\n",
    "    return train_accuracy_list, val_accuracy_list, train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(lr, num_unfreeze, num_epochs):\n",
    "    model_conv = torchvision.models.resnet18(pretrained=True)  # download ResNet18\n",
    "    for i, param in enumerate(model_conv.parameters()):\n",
    "        if i < 62 - num_unfreeze:  \n",
    "            param.requires_grad = False\n",
    "\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, 1024) \n",
    "    model_conv.fc2 = nn.Linear(1024, 32) \n",
    "    model_conv.fc3 = nn.Linear(32, 5)\n",
    "\n",
    "    model_conv = model_conv.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=lr)\n",
    "\n",
    "    return train_model(model_conv, criterion, optimizer_conv, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_experiment_results_to_file(filename, results_dict):\n",
    "    with open(filename, 'w+') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(results_dict.keys())\n",
    "        num_rows = len(list(results_dict.values())[0])\n",
    "        for i in range(num_rows):\n",
    "            row = []\n",
    "            for key in results_dict.keys():\n",
    "                row.append(float(results_dict[key][i]))\n",
    "            writer.writerow(row)\n",
    "        print('Wrote {} rows to file.'.format(num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.001, 0.0001, 0.00001]\n",
    "num_epochs = 1\n",
    "for lr in lr_list:\n",
    "    for num_unfreeze in range(5):\n",
    "        train_accuracy_list, val_accuracy_list, train_loss_list, val_loss_list = run_experiment(lr=lr, num_unfreeze=1, num_epochs=num_epochs)\n",
    "\n",
    "        results_filename = os.path.dirname(notebook_path) + \"/Workflow/experiments/lr={}_num_unfroze={}_epochs={}.csv\".format(lr, num_unfreeze, num_epochs)\n",
    "        results_dict = {\"train_accuracy\": train_accuracy_list, \"val_accuracy\": val_accuracy_list, \"train_loss\": train_loss_list, \"val_loss\": val_loss_list}\n",
    "        write_experiment_results_to_file(results_filename, results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}